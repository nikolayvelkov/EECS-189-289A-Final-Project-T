{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvKbtZSzAzMd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd  \n",
    "from scipy.stats import expon\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rzjipepCB2E"
   },
   "source": [
    "# Project T Final: Linear Regression\n",
    "\n",
    "This coding assignment will focus on understanding linear regression across various applications. We start with 1D functions to understand the basic application as well as a little on the data science terms for fitting. Then we move on to look at ordinary least squares and linear regression. We move onto a topic called featurization where we look at various properties or *features* extended from EE16A concepts. Finally, we start thinking about the topic of classification where you can visualize how separating data into different groups or *classes* would work (for binary and multiclass). \n",
    "\n",
    "Quiz questions are asked throughout the notebook, so make sure to take your time and think about your responses to these as they help you understand the examples and visualizations more. \n",
    "\n",
    "At the end of this notebook, we outline pros and cons for linear regression and when more complex models are necessary to reach an optimal solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UszyayvESw_d"
   },
   "source": [
    "# Linear Regression 1D Functions\n",
    "\n",
    "We will first go through an example of a line with random noise added and try to work backward and find its slope and intercept. Then we start using a 1d raw dataset and fit a linear model to that and move on to understand what $r^2$ represents to understand linear models from a data science perspective. \n",
    "\n",
    "Using the previous parts, we apply correlation coefficient perspective on a dataset and visualize loss to see that the solution is the most optimal! Finally, we see how to easily use sklearn linear regression in a similar fashion to previous parts and how to fit an exponential curve to the linear regression model by writing as: $\\log(y)= m\\log(x) + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VT-ogPDwaY_F"
   },
   "source": [
    "Remember from the note that we can arrive to the slope and intercept equations as shown below: \n",
    "\n",
    "$b = \\frac{\\sum y_{i} - m \\sum x_{i}}{n}$\n",
    "\n",
    "$m = \\frac{\\sum x_{i}y_{i} - \\sum y_{i} \\sum x_{i} / n}{\\sum x_{i}^2 - (\\sum x_{i})^2 / n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o48gcIsdaf9t"
   },
   "source": [
    "## Noisy line data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "4lcza00lajxS",
    "outputId": "86bb37c9-3eed-4fbf-f2c6-5e5f144ddac8"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# TODO: vary the range of numbers used to see how the results change -- write your observations in the cell below\n",
    "#     hint: try single digit, double digit, triple digit and compare\n",
    "#     hint: change slope and intercept values to see change in results and r_squared\n",
    "# START\n",
    "\n",
    "# END\n",
    "\n",
    "# original equation\n",
    "print(\"Original equation: we're working backward from a dataset of x and y to find m and b\")\n",
    "print(\"y = m*x + b where m =\", m, \"and b =\", b)\n",
    "\n",
    "# add noise\n",
    "x += np.random.randn(len(x))*3\n",
    "y += np.random.randn(len(y))*3\n",
    "\n",
    "# turn into numpy arrays\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "#  plot raw data\n",
    "plt.scatter(x,y)\n",
    "plt.title(\"Noisy line\")\n",
    "plt.show()\n",
    "\n",
    "# TODO: write the calculations for the slope and intercept using information from above\n",
    "#     hint: use numpy dot, sum, and mean\n",
    "def calculateSlope(x, y):\n",
    "    # START\n",
    "\n",
    "    # END\n",
    "\n",
    "def calculateIntercept(x, y, m):\n",
    "    # START\n",
    "\n",
    "    # END\n",
    "    \n",
    "m = calculateSlope(x, y)\n",
    "b = calculateIntercept(x, y, m)\n",
    "\n",
    "print('calculated m = ', m)\n",
    "print('calculated b = ', b)\n",
    "\n",
    "# best fit y = mx + b line\n",
    "yhat = m*x + b\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yhat, 'r')\n",
    "plt.title(\"Best Fit\")\n",
    "plt.show()\n",
    "\n",
    "#  R^2 value (you can ignore the code, just understand what the r_squared indicates in the plot)\n",
    "def calculateRSquared(y, yhat):\n",
    "    res = (y - yhat).dot(y - yhat)\n",
    "    tot = (y - y.mean()).dot(y - y.mean())\n",
    "    return res, tot\n",
    "SSres, SStot = calculateRSquared(y, yhat)\n",
    "r_squared = 1 - SSres/SStot\n",
    "\n",
    "print('r_squared = ', r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCGWhmMkaotF"
   },
   "source": [
    "### TODO\n",
    "Observations on varying number of data points used: \n",
    "\n",
    "*Answer*\n",
    "\n",
    "Try to determine around what range of datapoints starts reporting an accurate answer. Also describe at what point you consider the linear fit \"accurate\" (if you understand r_squared, then answer with this):\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In later sections, we describe a term called \"bias\" which is derived from the error in approximating real problem. For example here, the bias defines the difference between the true values of slope/intercept and the calculated values we found. We discuss and use this in future sections of featurization and classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtUkezJ5atv4"
   },
   "source": [
    "## 1D dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "DiMB2ATEavIw",
    "outputId": "4069e2d0-c6c0-4276-96be-a4717a614132"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# TODO: open the csv file \"data_1d.csv\" and read in x,y data\n",
    "# START\n",
    "\n",
    "# END\n",
    "\n",
    "# add noise\n",
    "x += np.random.randn(len(x))*10\n",
    "y += np.random.randn(len(y))*10\n",
    "    \n",
    "# turn into numpy arrays\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "#  plot raw data\n",
    "plt.scatter(x,y)\n",
    "plt.title('1D data')\n",
    "plt.show()\n",
    "\n",
    "# TODO: calculate slope and intercept for new dataset\n",
    "# START\n",
    "\n",
    "# END\n",
    "\n",
    "print('calculated m = ', m)\n",
    "print('calculated b = ', b)\n",
    "\n",
    "# best fit y = mx + b line\n",
    "yhat = m*x + b\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yhat, 'r')\n",
    "plt.title(\"Best Fit\")\n",
    "plt.show()\n",
    "\n",
    "#  R^2 value\n",
    "SSres, SStot = calculateRSquared(y, yhat)\n",
    "r_squared = 1 - SSres/SStot\n",
    "\n",
    "print('r_squared = ', r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25B-3x7Aa6d6"
   },
   "source": [
    "## Understanding R_Squared and Correlation: \n",
    "\n",
    "In the past couple questions, you answered what you thought r_squared means. Here, we'll explain a little further. First though, we'll look at some more general datasets to see when linear regression models make sense vs. not. \n",
    "\n",
    "In data science, the value $r^2$ represents the squared value of the correlation coefficient which visually shows correlation between x and y data. This r value is known as Pearson's product moment correlation coefficient. This coefficient is based on standard units with values ranging from -1 to 1 where $r = 1$ would have a scatter perfectly straight line sloped up and $r = -1$ scattered perfectly sloping down. In between, $r = 0$ shows no linear association, i.e. uncorrelated x and y data.\n",
    "\n",
    "Thus $r^2$ is proportional to how far a predicted point deviates from its true value in y and can be modeled by the linear regression of x and y. For example, the answers to which models are most conducive to linear regression have a higher r coefficient vs. not conducive have lower r coefficient. \n",
    "\n",
    "TODO: After reading the above explanation, discuss in your own terms what you have learned and what you think this coefficient measures: \n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "w_WHhBeRa9L8",
    "outputId": "650e4beb-bf7b-4ba8-b777-cb3fdbda78ac"
   },
   "outputs": [],
   "source": [
    "# Just noise\n",
    "np.random.seed(50)\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "x1, y1 = np.random.randn(2, 100)\n",
    "plt.scatter(x1, y1, alpha = 0.75);\n",
    "print(np.corrcoef(x1, y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "VXWsuq4Xa_og",
    "outputId": "5e52235c-e673-4065-8cc1-9a05e25aca94"
   },
   "outputs": [],
   "source": [
    "# Strong linear\n",
    "np.random.seed(50)\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "x2 = np.linspace(-3, 3, 100)\n",
    "y2 = x2*0.5 - 0.5 + np.random.randn(100)*0.35\n",
    "plt.scatter(x2, y2, alpha = 0.75);\n",
    "print(np.corrcoef(x2, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ek5WoSnNbAkl"
   },
   "outputs": [],
   "source": [
    "# Strong non-linear\n",
    "np.random.seed(50)\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "x3 = np.linspace(-3, 3, 100)\n",
    "y3 = 2*np.sin(x3 - 1.5) + np.random.randn(100)*0.4\n",
    "plt.scatter(x3, y3, alpha = 0.75);\n",
    "print(np.corrcoef(x3, y3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qgLrTjODbCps"
   },
   "outputs": [],
   "source": [
    "# Unequal spread\n",
    "np.random.seed(50)\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "x4 = np.linspace(-3, 3, 100)\n",
    "y4 = x4/3 + np.random.randn(100)*(x4)/1.75\n",
    "plt.scatter(x4, y4, alpha = 0.75);\n",
    "print(np.corrcoef(x4, y4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYI0bSz9bHfL"
   },
   "source": [
    "### TODO\n",
    "Please comment below which models are most conducive to a linear regression model and why:\n",
    "\n",
    "*Answer*\n",
    "\n",
    "For those not conducive to linear regression, please explain what might fit better (just brainstorm methods). If there is no solution, mention that and why: \n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbT8bYDMb-Bq"
   },
   "source": [
    "## Sklearn Linear Regression Model\n",
    "\n",
    "Next we repeat with the sklearn linear regression model to understand how to use ready-made libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qTdD9DyLcAVa"
   },
   "outputs": [],
   "source": [
    "# TODO: Load the diabetes dataset\n",
    "#     hint: look at sklearn datasets docs\n",
    "# START\n",
    "\n",
    "# END\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# TODO: Create linear regression object\n",
    "#       Train the model using diabetes_X_train,diabetes_y_train\n",
    "#       Make predictions using diabetes_X_test\n",
    "#     hint: look into sklearn linear regression docs\n",
    "# START\n",
    "\n",
    "# END\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='blue')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='red')\n",
    "plt.title('Diabetes Linear Regression')\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# R^2 value: 1 is perfect prediction\n",
    "print('R_squared: %.2f'\n",
    "      % r2_score(diabetes_y_test, diabetes_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JPmhcuacEx4"
   },
   "source": [
    "## Sklearn Exponential Curve Fit\n",
    "\n",
    "Finally, we fit an exponential curve by transforming the data to a linear regression model by writing as: $\\log(y)= m\\log(x) + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "-AZKdgwPcFw3",
    "outputId": "464c611e-3d17-4909-8d47-da6840d9756b"
   },
   "outputs": [],
   "source": [
    "# create exponential curve\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "x = np.linspace(expon.ppf(0.01), expon.ppf(0.99), 50)\n",
    "y = expon.pdf(x)\n",
    "ax.plot(x, y, 'r-', alpha=0.6, label='expon pdf')\n",
    "plt.title(\"Exponential PDF\")\n",
    "\n",
    "# add noise\n",
    "x += np.exp(np.random.randn(len(x)))/20\n",
    "y += np.exp(np.random.randn(len(y)))/30\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.show()\n",
    "\n",
    "# TODO: Change data to allow linear regression fit for exponential function\n",
    "# START\n",
    "\n",
    "# END\n",
    "\n",
    "# turn into numpy arrays\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "# TODO: calculate slope and intercept for new dataset\n",
    "# START\n",
    "\n",
    "# END\n",
    "\n",
    "print('calculated m = ', m)\n",
    "print('calculated b = ', b)\n",
    "\n",
    "# best fit y = mx + b line\n",
    "yhat = m*x + b\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yhat, 'r')\n",
    "plt.title('Best Fit')\n",
    "plt.show()\n",
    "\n",
    "#  R^2 value\n",
    "SSres, SStot = calculateRSquared(y, yhat)\n",
    "r_squared = 1 - SSres/SStot\n",
    "\n",
    "print('r_squared = ', r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duz8ljyWcLI8"
   },
   "source": [
    "Your r_squared should come out to around 0.8-0.9. This section was to just show that transformations to the data can lead to better fits via linear regression. \n",
    "\n",
    "We'll discuss more about what this transformation means in our later section in featurization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24eJOxg1OCnV"
   },
   "source": [
    "# Linear Regression OLS and 3D plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZw4J1d-OzzU"
   },
   "source": [
    "## Ordinary Least Squares Regression without SKLearn\n",
    "\n",
    "We can start using the normal equations:\n",
    "\n",
    "$$\\hat{\\theta} = \\left( \\mathbb{X}^T \\mathbb{X} \\right)^{-1} \\mathbb{X}^T \\mathbb{Y}$$\n",
    "\n",
    "We can compute $\\hat{\\theta}$ by direction using matrix inversion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hUfPc6YO3Wd"
   },
   "source": [
    "### Ordinary least squares (OLS)\n",
    "\n",
    "In the note, we concluded that the optimum weight $w^{*}$ that solves $\\min_{w} ||\\textbf{X}\\textbf{w}-\\textbf{y}||_2^2$ satisfies:\n",
    "\n",
    "$$X^TXw_{ols}^* = X^Ty$$\n",
    "\n",
    "If X is full rank, then $X^TX$ is as well (assuming n ≥ d), so we can solve for a unique solution:\n",
    "\n",
    "$$w_{ols}^* = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKtSo1ZnO_-q"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv, solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nd1klQ6ePCT2"
   },
   "source": [
    "## TODO ##\n",
    "Write the `least_squares_by_inv`  function  to solve the linear systems: $$w_{ols}^* = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmvA4hjgPGW2"
   },
   "outputs": [],
   "source": [
    "def least_squares_by_inv(X, Y):\n",
    "  # START\n",
    "    return ...\n",
    "  # END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JakpOhtcPFqW"
   },
   "source": [
    "A more efficient way to solve the normal equations is using the `least_squares_by_solve` function to solve the linear systems:\n",
    "\n",
    "$$\n",
    "A \\theta = b\n",
    "$$\n",
    "\n",
    "where $A=\\mathbb{X}^T \\mathbb{X}$ and $b=\\mathbb{X}^T \\mathbb{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBeu-dBWPLSs"
   },
   "outputs": [],
   "source": [
    "def least_squares_by_solve(X, Y):\n",
    "  # START\n",
    "    return ...\n",
    "  # END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwhjmcQQPP7-"
   },
   "source": [
    "## House Price Prediction\n",
    "\n",
    "The real estate markets, like those in Sydney and Melbourne, present an interesting opportunity for data analysts to analyze and predict where property prices are moving towards. Prediction of property prices is becoming increasingly important and beneficial. Property prices are a good indicator of both the overall market condition and the economic health of a country. Considering the data provided, we are wrangling a large set of property sales records stored in an unknown format and with unknown data quality issues.\n",
    "\n",
    "(Data from Kaggle: https://www.kaggle.com/shree1992/housedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "p3ygOclhPZQb",
    "outputId": "481bcdcb-7957-4384-a86e-97205fb7b6e1"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCDH_PoqP0Kl"
   },
   "source": [
    "### Price\n",
    "We begin by examining a raincloud plot (a combination of a KDE, a histogram, a strip plot, and a box plot) of our target variable `Price`. At the same time, we also take a look at some descriptive statistics of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "pBuZkZOdP2Sa",
    "outputId": "a7cd9349-08ae-4309-d841-8f1abcb9f100"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2)\n",
    "sns.distplot(\n",
    "    data['price'], \n",
    "    ax=axs[0]\n",
    ")\n",
    "sns.stripplot(\n",
    "    data['price'], \n",
    "    jitter=0.4, \n",
    "    size=3,\n",
    "    ax=axs[1],\n",
    "    alpha=0.3\n",
    ")\n",
    "sns.boxplot(\n",
    "    data['price'],\n",
    "    width=0.3, \n",
    "    ax=axs[1],\n",
    "    showfliers=False,\n",
    ")\n",
    "\n",
    "# Align axes\n",
    "spacer = np.max(data['price']) * 0.05\n",
    "xmin = np.min(data['price']) - spacer\n",
    "xmax = np.max(data['price']) + spacer\n",
    "axs[0].set_xlim((xmin, xmax))\n",
    "axs[1].set_xlim((xmin, xmax))\n",
    "\n",
    "# Remove some axis text\n",
    "axs[0].xaxis.set_visible(False)\n",
    "axs[0].yaxis.set_visible(False)\n",
    "axs[1].yaxis.set_visible(False)\n",
    "\n",
    "# Put the two plots together\n",
    "plt.subplots_adjust(hspace=0)\n",
    "\n",
    "# Adjust boxplot fill to be white\n",
    "axs[1].artists[0].set_facecolor('white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg1-jzr8T4sW"
   },
   "source": [
    "### TODO \n",
    "Let take a look at some descriptive statistics of this variable. (Hint: Use the `describe`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDGILwAUP5nT",
    "outputId": "769c0a90-e644-4dc6-e16e-fa185bff3d09"
   },
   "outputs": [],
   "source": [
    "#START\n",
    "...\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_QsSXw_P8Ui"
   },
   "source": [
    "### TODO\n",
    "What do you find from the plot and descriptive statistics of this variable?\n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GEyO-QBQDHE"
   },
   "source": [
    "### TODO\n",
    "Let try to see the plot below 75% of the points (75% is 6.575000e+05). You could use the code we provided above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "7Ev5D_BwP7wf",
    "outputId": "651f8bda-1684-4511-b80e-58b0ec8bf724"
   },
   "outputs": [],
   "source": [
    "#START\n",
    "\n",
    "...\n",
    "\n",
    "#END\n",
    "# Align axes\n",
    "spacer = np.max(data1['price']) * 0.05\n",
    "xmin = np.min(data1['price']) - spacer\n",
    "xmax = np.max(data1['price']) + spacer\n",
    "axs[0].set_xlim((xmin, xmax))\n",
    "axs[1].set_xlim((xmin, xmax))\n",
    "\n",
    "# Remove some axis text\n",
    "axs[0].xaxis.set_visible(False)\n",
    "axs[0].yaxis.set_visible(False)\n",
    "axs[1].yaxis.set_visible(False)\n",
    "\n",
    "# Put the two plots together\n",
    "plt.subplots_adjust(hspace=0)\n",
    "\n",
    "# Adjust boxplot fill to be white\n",
    "axs[1].artists[0].set_facecolor('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NQ9LspbQHeS"
   },
   "source": [
    "### Price vs Bedrooms and Bathrooms\n",
    "Nex we want to see how the number of bedrooms and bathrooms would affect the house price. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1QyulIiSEZ2"
   },
   "source": [
    "### TODO\n",
    "Let make an DataFrame only include price, bedrooms and bathrooms. Let set the x1 be the number of bedrooms, x2 be the number of bathrooms and y be the number of price. Since the price is high let divide the price by 1,000,000.\n",
    "\n",
    "Here also we only care about the price below 6.575000e+05!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "fW24G8MQUV1k",
    "outputId": "c438448f-2dc4-461b-e759-cd070869e78b"
   },
   "outputs": [],
   "source": [
    "#START\n",
    "data = ...\n",
    "x1 = ...\n",
    "x2 = ...\n",
    "y = ...\n",
    "### make a new dataframe contain x1, x2, and y\n",
    "sample_data = ...\n",
    "### remove the points with price equal to 0\n",
    "sample_data = ...\n",
    "sample_data\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm4u-iYNTMfc"
   },
   "source": [
    "### Testing on Sample Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1akZhSa9SA4G"
   },
   "source": [
    "### TODO\n",
    "Let's quickly test two linear regressions on our sample dataset:\n",
    "$$w_{ols}^* = (X^TX)^{−1}X^Ty$$ and \n",
    "$$\n",
    "A \\theta = b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lRBsBqPPf7o"
   },
   "outputs": [],
   "source": [
    "# Let make the x be number of x1 and x2, and y be all the number of y\n",
    "# START\n",
    "x = ...\n",
    "y = ...\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaOJvwgZPh81"
   },
   "source": [
    "First let use $$w_{ols}^* = (X^TX)^{−1}X^Ty$$ to calculate our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgsPTC4iPkfK",
    "outputId": "fe7c6324-e67c-4256-c834-1e94c6a6be8f"
   },
   "outputs": [],
   "source": [
    "# START\n",
    "w_hat = ...\n",
    "w_hat\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR4yghOVPshG"
   },
   "source": [
    "Then let use $$\n",
    "A \\theta = b\n",
    "$$ to calculate our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mbb2YVytPvLm",
    "outputId": "40411c38-d473-44c7-9eec-ea54b30d0283"
   },
   "outputs": [],
   "source": [
    "# START\n",
    "theta_hat = ...\n",
    "theta_hat\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMV_accQPx-D"
   },
   "source": [
    "They both agree! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sJ58osAQL1n"
   },
   "source": [
    "### TODO\n",
    "Next, we visualize the association between Price, Bedrooms, and Bathrooms.\n",
    "\n",
    "(hint: Use `go.Scatter3d` https://plotly.github.io/plotly.py-docs/generated/plotly.graph_objects.Scatter3d.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "RdFAZtb_QN74",
    "outputId": "c0e1040c-7e36-41d3-bd8f-ba0643dabe7f"
   },
   "outputs": [],
   "source": [
    "#Let make 3D plot of all the data points.\n",
    "# START\n",
    "fig = go.Figure()\n",
    "\n",
    "data_scatter = go.Scatter3d(x=..., y=..., z=..., \n",
    "                            mode=\"markers\",\n",
    "                            marker=dict(size=2))\n",
    "fig.add_trace(data_scatter)\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), \n",
    "                  height=600)\n",
    "fig\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark ###\n",
    "Note that our data isn't perfectly linearly separable. We will see that this is true for most real datasets for two reasons: 1) we don't have enough or the right features, so we will be forced to underfit the model 2) there will most of the time be inherent noise in the dataset as well as outliers in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aQrmDLqQQYj"
   },
   "source": [
    "## Using SKLearn\n",
    "\n",
    "Scikit Learn, or as the cool kids call it sklearn (pronounced s-k-learn), is an large package of useful machine learning algorithms. We will use the `LinearRegression` model in the [`linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) module.  The fact that there is an entire module with many different models within the `linear_model` module might suggest that we have a lot to cover still (we do!).  \n",
    "\n",
    "**What you should know about `sklearn` models:**\n",
    "\n",
    "1. Models are created by first building an instance of the model:\n",
    "```python\n",
    "model = ModelType(args)\n",
    "```\n",
    "1. You then fit the model by calling the **fit** function passing in data:\n",
    "```python\n",
    "model.fit(df[['x1' 'x2']], df[['y']])\n",
    "```\n",
    "1. You then can make predictions by calling **predict**:\n",
    "```python\n",
    "model.predict(df2[['x1' 'x2']])\n",
    "```\n",
    "\n",
    "The neat part about sklearn is most models behave like this.  So if you want to try a cool new model you just change the class of mode you are using. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPvvotHoQT0h"
   },
   "source": [
    "We import the `LinearRegression` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckasqc6DQRMf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3uicjbYQYEz"
   },
   "source": [
    "### TODO\n",
    "Create an instance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKsGK1_vQZ2W"
   },
   "outputs": [],
   "source": [
    "# START\n",
    "model = ...\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDwrOft7QcHm"
   },
   "source": [
    "### TODO\n",
    "Fit the model by passing it the $X$ and $Y$ data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFZETlYZQd8x",
    "outputId": "62413328-870b-4314-a22b-320ae0207aa1"
   },
   "outputs": [],
   "source": [
    "# START\n",
    "...\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgnsUcp8Qf3W"
   },
   "source": [
    "### TODO\n",
    "Make some predictions and even save them back to the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "KFw-NgpwQhym",
    "outputId": "ccf33ce7-833d-4464-b3df-7eb857d5887d"
   },
   "outputs": [],
   "source": [
    "# START\n",
    "sample_data['y_hat'] = ...\n",
    "sample_data\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS4Ph9OuQj-f"
   },
   "source": [
    "This function could make a plane on the 3D plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-P8vEjBjQmCC"
   },
   "outputs": [],
   "source": [
    "def plot_plane(f, X, grid_points = 30):\n",
    "    u = np.linspace(X[:,0].min(),X[:,0].max(), grid_points)\n",
    "    v = np.linspace(X[:,1].min(),X[:,1].max(), grid_points)\n",
    "    xu, xv = np.meshgrid(u,v)\n",
    "    X = np.vstack((xu.flatten(),xv.flatten())).transpose()\n",
    "    z = f(X)\n",
    "    return go.Surface(x=xu, y=xv, z=z.reshape(xu.shape),opacity=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cBzdl_eQmwO"
   },
   "source": [
    "### TODO\n",
    "Plot the data and plane (hint: use the funtion we provided above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "F4_zJArFQq_u",
    "outputId": "a3805dd3-3f2e-4c8f-d7ac-622438ae00e3"
   },
   "outputs": [],
   "source": [
    "# START\n",
    "fig = go.Figure()\n",
    "fig.add_trace(data_scatter)\n",
    "fig.add_trace(...)\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), \n",
    "                  height=600)\n",
    "fig.show()\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkZJnyQUQtpS"
   },
   "source": [
    "### TODO\n",
    "What do you find from this graph?\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub362kMvG_5C"
   },
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "## What is categorical data?\n",
    "\n",
    "**Categorical data** are variables that are classified by labels rather than strictly numerical values. Often these variables are called _nominal_ variables. For example, a \"pet\" variable can have values like \"dog\", \"cat\", or \"iguana\". \n",
    "\n",
    "As you may know, data sets aren't always in numerical form. Similar to categorizing your classes by subject, or the clothes you wear by occasion, humans have a natural tendancy to associate data with nominal variables. \n",
    "\n",
    "Other forms of data include numerical, as we have seen before, and ordinal data, which are categorical data which have rankings or order. Ex: star ratings on yelp.\n",
    "\n",
    "### So how do computers see and use categorical data?\n",
    "\n",
    "Some algorithms can work with categorical data directly, for example, decision trees (which you will learn about later in ML) Our job now as avid machine learning enthusiasts is to translate categorical data into a way the majority of machine learning algorithms can efficiently provide modelling and analysis, which brings us to our module topic: One Hot Encoding.\n",
    "\n",
    "## One Hot Encoding\n",
    "\n",
    "One Hot encoding expands the columns of a categorical dataset to include the categories as columns in a numerical way.\n",
    "\n",
    "The term \"One Hot\" comes from how categories are indicated in this form, with the row record's are lit up as a '1' if it falls into the category of question, and 0 otherwise.\n",
    "\n",
    "![Tabular image of One Hot Encoding](images/one_hot_state.png)\n",
    "\n",
    "While this is a small example, one hot encoding can span several categories and simple but robust enough to be used commonly in various fields of machine learning and data science. For example, Natural Language Processing sometimes will take tweets (each row would be one recorded tweet), and vectorize words similar to how we one-hot encode catagories!\n",
    "\n",
    "Fun Fact: The term one-hot encoding comes from a digital circuit encoding of a categorical state as particular \"hot\" wire.\n",
    "\n",
    "![Wire Image of One Hot Encoding](images/one_hot_encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM0XFO7AIuiw"
   },
   "source": [
    "### The Process\n",
    "\n",
    "One hot encoding is a way to reframe your existing data. \n",
    "\n",
    "5 Steps to manually produce One hot encoding:\n",
    "1. Clean the dataset and create a column for the category. This table with the original catagory column will be called 'Table A'.\n",
    "2. Find the set of unique labels in a category from Table A.\n",
    "3. For each unique label, create columns in a new dataframe, Table B. (If there are multiple catagorical columns to encode, common practice is to rename these new columns in the form 'category_value - ex: 'species_human', 'species_dog') Fill these columns with the default value, 0.\n",
    "4. For each non-categorical column (the numerical columns where one-hot encoding is not needed), append it to this to the new dataframe, Table B.\n",
    "5. Loop through each record (row value) of the Table A. For each category, you want to change the encoded value in Table B to '1'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNBUgMBXKYxt"
   },
   "source": [
    "## Let's put what we learned into action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKJpPfswVBZh"
   },
   "source": [
    "## Part A: Simple Encoding Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSdoeEqbHfWf"
   },
   "source": [
    "\n",
    "### Part 1: Manual Encoding\n",
    "One weekend the staff of EE16ML decide to open a fruit stand to help raise funds to support Oski's career as a computer scientist. The staff is given a catalogue of fruits they are able to source for their new pop up fruit shop.\n",
    "\n",
    "The fruit column represents the label that the fruit has - 'apple', for example. The price column is a float representing the $USD of a pound of the corresponding fruit. The season is the best season for the fruit to grow between summer and winter. The rating column represents the popularity of the fruit among Berkeley students out of 5.\n",
    "\n",
    "The EE16ML staff want to reframe the existing data so that they can run some code on it. As a go-getter student assistant, you volunteer to preprocess the table and one-hot encode the catagorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "wUgrFHBRG7F2",
    "outputId": "427dd58a-dd09-4879-c2b5-579c6976b912"
   },
   "outputs": [],
   "source": [
    "## A Simple Example\n",
    "# run this cell to create the dataset\n",
    "fruit_info = pd.DataFrame(\n",
    "    data={'fruit': ['apple', 'orange', 'banana', 'raspberry'],\n",
    "          'price': [5, 6, 2, 3],\n",
    "          'season': ['winter', 'winter', 'summer', 'summer'],\n",
    "          'rating': [4, 2, 3, 5]\n",
    "          })\n",
    "# display fruit_info\n",
    "fruit_info\n",
    "\n",
    "## data example source: data 100 lab 2 https://data100.datahub.berkeley.edu/user/jiyoojeong/notebooks/sp20/lab/lab02/lab02.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3IgRtPUSTLV"
   },
   "source": [
    "### TODO\n",
    "Now we want to manually one hot encode this table using the process we just learned.\n",
    "\n",
    "_Hint: It might help to look at the sanity check questions!_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "aWlmCEe8Ul4l",
    "outputId": "d9ac656a-2cd4-4602-a5e3-ac815eece48a"
   },
   "outputs": [],
   "source": [
    "# you may not use any libraries outside of pandas for this section.\n",
    "fruit_one_hot = pd.DataFrame()\n",
    "\n",
    "# TODO: Your code here.\n",
    "# Hint: think about what you would like to do at each step, and how many times\n",
    "# the process will be repeated.\n",
    "# Hint 2: A simple pandas function to add new empty columns is df.reindex!\n",
    "\n",
    "# alternative, but working solutions, will be accepted.\n",
    "\n",
    "## START\n",
    "\n",
    "# END.\n",
    "\n",
    "# categorize and encode\n",
    "for i in fruit_info.index.values:\n",
    "  fruit_name = fruit_info['fruit'][i]\n",
    "  season = fruit_info['season'][i]\n",
    "  one_hot_vector = [1 if fruit_name in c or season in c else 0 for c in new_columns]\n",
    "  fruit_one_hot.loc[i, new_columns] = one_hot_vector\n",
    "\n",
    "\n",
    "# display table\n",
    "fruit_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC9MxxMiIrbr"
   },
   "source": [
    "Sanity Check: \n",
    "- How many columns should your new table have? How many are columns from the catagorical variables? _your answer here_\n",
    "- What should the columns be? _your answer here_\n",
    "- Does your newly encoded table still have the same data points? Briefly justify why. _your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5Gno7rASFPt"
   },
   "source": [
    "#### Decoding the one-hot encode\n",
    "\n",
    "To help solidify your understanding of one-hot encoding, think about how the script to undo a one-hot encoding process would work. Describe in detail what this process would look like in pseudocode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyifPNeqVm6K"
   },
   "source": [
    "_Your pseudo code here_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzyVbbn4FIRW"
   },
   "source": [
    "### Part 2: Pandas One Hot Encoding\n",
    "\n",
    "The previous manual method is actually pretty inefficient for large data, as we need to loop in through the data multiple times and determine new columns. Using pandas to hold your data in dataframes however allows us to actually utilise one of their library functions to one hot encode catagories efficiently and effectively!\n",
    "\n",
    "You can use Pandas.get_dummies to get the one hot encoded version of the dataframe.\n",
    "\n",
    "Try running the cell below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "410bZF1tFa5T",
    "outputId": "c0af28b1-5090-4b36-f185-c01733b5cac0"
   },
   "outputs": [],
   "source": [
    "# returns the one hot encoded table of only column 'fruit'\n",
    "pd.get_dummies(fruit_info['fruit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "OmUjlfIYFtld",
    "outputId": "9e9a4eb2-f111-40da-c52e-d50df63cc9aa"
   },
   "outputs": [],
   "source": [
    "# returns the full dataframe with one hot encoded columns\n",
    "pd.get_dummies(fruit_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCCOfZVMH8wP"
   },
   "source": [
    "### Part 3: Using SKLearn's OneHotEncoder\n",
    "\n",
    "For larger or more complex datasets, SKLearn provides a One Hot Encoder through it's preprocessing kit. Find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "\n",
    "[Data 100's textbook chapter on one hot encoding](https://www.textbook.ds100.org/ch/14/feature_one_hot.html) also has a great example of using the DictVectorizer from the feature_extraction library instead of OneHotEncoder for similar results if you want an additional perspective of one hot encoding using SKLearn. \n",
    "\n",
    "\n",
    "* Note: Using the OneHotEncoder will encode the entire dataframe you input as one-hot categories, even numerical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMtlkdrHH8Hf"
   },
   "outputs": [],
   "source": [
    "## SKLearn's OneHotEncoder\n",
    "\n",
    "# load library\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0G0W_vJKZAd"
   },
   "source": [
    "First we observe the simple example of encoding a matrix with binary one-hot encoding. The following sample data has two catagories, one for an individual's gender and another for a the order of arrival at a restaurant.\n",
    "\n",
    "#### From the Docs\n",
    "> \"Encode categorical features as a one-hot numeric array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the sparse parameter. By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually. This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.\"\n",
    "\n",
    "\n",
    "Review the documentation for the SKLearn OneHotEncoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWWyC2WcURaK"
   },
   "source": [
    "### TODO\n",
    "For the following cells where prompted, run and understand what is happening, and utilise the library functions to obtain desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7Jbc9cwJxIl",
    "outputId": "902f9c01-cce1-4d5c-b980-0191f91a64aa"
   },
   "outputs": [],
   "source": [
    "# A simple example : courtesy of scikit-learn.org\n",
    "\n",
    "# create and fit the encoder to the dataframe\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc.fit(fruit_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CHqTPV_Ldv6",
    "outputId": "634ddb41-7244-49de-a606-7055ab02adb6"
   },
   "outputs": [],
   "source": [
    "# TODO: Display the created catagories:\n",
    "# START\n",
    "\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOahXEy6LkoZ",
    "outputId": "28882030-847b-4c36-b53a-14edaeebeb5a"
   },
   "outputs": [],
   "source": [
    "# get vectorized one-hots from inputs\n",
    "enc.transform([['apple', 2, 'winter', 4], ['raspberry', '5', 'spring', '2']]).toarray()\n",
    "# notice how since spring was not trained on, it does not get encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9yFIkD9L3qZ",
    "outputId": "1885f984-74dc-47b2-9171-6b5c5dc4dfb9"
   },
   "outputs": [],
   "source": [
    "# TODO: return a catagory from a the binary vectors given.\n",
    "# START\n",
    "\n",
    "# END\n",
    "# Hint: you should be getting the same inputs as the previous cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agNV0lzlMJzp",
    "outputId": "1354199e-b40f-4451-cb6e-d6a7c488c00d"
   },
   "outputs": [],
   "source": [
    "# returns the feature names\n",
    "enc.get_feature_names(['fruit', 'price', 'season', 'rating'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_keTlR2Pq0r"
   },
   "source": [
    "\n",
    "\n",
    "> What step of the manual featurization does this resemble? _your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWk9FUdGOavl"
   },
   "source": [
    "## So what do you do if you have labels in your y values?\n",
    "\n",
    "Instead of using the OneHotEncoder, SKLearn has an additional encoder called the [Label Encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) as well as the [Label Binarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer). This is used when the labels are catagorical in nature, similar to the ideology of the OneHotEncode.\n",
    "\n",
    "However, since labels may be numerous beyond 2, but the array needs to remain a 1-d array, encoding labels with the Label Encoder in increasing numerical order helps create a numerical representation of the data to feed into advanced machine learning algorithms. The Label Binarizer on the otherhand, uses a one-vs-all system, where one label is encoded as '1', while the rest are encoded as '0'.\n",
    "\n",
    "Look forward to later this week where we will be working with binary classification !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yffpueoJJRh"
   },
   "source": [
    "## Part B: Working with more sophisticated data + Linear Regression!\n",
    "\n",
    "You spend the weekend bingeing movies on netflix, and are curious about what factors can make a movie really successful. You gather a dataset of three decades of released films to explore your question: Can we predict the gross of a movie from other factors of production?\n",
    "\n",
    "Explore the provided [movie industry dataset](https://www.kaggle.com/danielgrijalvas/movies) and answer the questions below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "iSBlQZGUJggY",
    "outputId": "f58d06b3-dd70-432f-edaa-036cfa08fa4a"
   },
   "outputs": [],
   "source": [
    "## A bit more complex example\n",
    "# load google playstore user app data\n",
    "movies = pd.read_csv('/content/sample_data/movies_dat.csv')\n",
    "# courtesy of Lavanya Gupta // source: https://www.kaggle.com/lava18/google-play-store-apps\n",
    "cols = movies.columns\n",
    "# code for cleaning the dataset and removing NaN's and other errors in the data has been provided below\n",
    "# run cell to clean data and retrieve relevant columns. Original table can still be accessed using google_apps\n",
    "print(cols)\n",
    "\n",
    "# display top 5 rows of data\n",
    "movies.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcf4iYQgvyld"
   },
   "source": [
    "### TODO: Framing Questions\n",
    "1. For each of the following columns, state whether or not it is a numerical, ordinal, or categorical variable.\n",
    "\n",
    "*Answer*\n",
    "'budget' -  , 'company' -  , 'country' - , 'director' -  , 'genre' - , 'gross' - , 'name' -  , 'rating' - , 'released' - , 'runtime' - , 'score' -  , 'star' - , 'votes' - , 'writer' - , 'year' - \n",
    "\n",
    "\n",
    "2. Which columns seem relevant to exploring your question? (Select at least 3 to describe).\n",
    "\n",
    "*Answer*\n",
    "\n",
    "3. Which of the selected columns will you have to clean? Do some data types or phrasing have to be cleaned or changed to fit your needs better?\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9wZPlmAwy75"
   },
   "source": [
    "### Preprocessing and One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uiJxdTZwyLO"
   },
   "outputs": [],
   "source": [
    "# select columns of interest\n",
    "columns = ['budget', 'company', 'country', 'director', 'genre', 'score', 'name','rating', 'released', 'runtime', 'star', 'votes', 'writer','year']\n",
    "\n",
    "predict_this_column = ['gross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "X-tngB5vcB8v",
    "outputId": "bcd21f23-0b39-4e5b-b268-da55ef16b71e"
   },
   "outputs": [],
   "source": [
    "# For each column that is a catagorical variable, see how many possible values there can be.\n",
    "# If this is a large number, it can hinder performance and may not be as useful as you might have thought.\n",
    "for i, c in enumerate(columns):\n",
    "  ## Uncomment to view to histograms. This will take a while to run\n",
    "  #plt.subplot(4,4,i+1)\n",
    "  #plt.hist(movies[c], bins = 15)\n",
    "  #plt.title(c)\n",
    "  print(\"---- column: \", c , \"----\")\n",
    "  #print(movies[c].value_counts().head())\n",
    "  valcounts = movies[c].value_counts()\n",
    "  print('total #: ', len(valcounts))\n",
    "  print('most common value, freq: ', valcounts.index[0],',', valcounts[valcounts.index[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoxBFEa_4A_9"
   },
   "source": [
    "Some columns are left in string format rather than int/double type, which is what we need to use for numerical data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G__E4Mjc4an_"
   },
   "outputs": [],
   "source": [
    "# TODO: START\n",
    "# Hint: pandas provides a datetime type conversion using .to_datetime()\n",
    "# Hint: pandas also has a .astype() to change types of series.\n",
    "\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Q9QB17gMCBN"
   },
   "outputs": [],
   "source": [
    "# Optional: plot interesting relationships here while exploring.\n",
    "#movies_clean.plot('score', 'gross', 'scatter')\n",
    "#movies_clean.plot('year', 'gross', 'scatter')\n",
    "#movies_clean.plot('budget', 'gross', 'scatter')\n",
    "#movies_clean.plot('runtime', 'gross', 'scatter')\n",
    "#movies_clean.plot('genre', 'gross', 'scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-u7caSG5UMk"
   },
   "source": [
    "Next, we must one-hot encode the categorical variables.\n",
    "\n",
    "One hot encode the columns for the movies dataset and save it as movies_enc. It is best not to use the 'director', 'star', 'release date' and 'name' columns for efficiency.\n",
    "\n",
    "Try to find the most efficient solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "W17uc8275T51",
    "outputId": "14c980f1-1e68-4108-cb75-805ec4eab8d4"
   },
   "outputs": [],
   "source": [
    "gross = movies_clean['gross']\n",
    "# TODO: \n",
    "# Hint: Use the pandas.get_dummies function!\n",
    "# alternative, but working solutions, will be accepted.\n",
    "# START\n",
    "\n",
    "# END\n",
    "movies_enc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUaJ3JPfWNr6"
   },
   "source": [
    "With this preprocessed data, we can now perform linear regression find the line of best fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrCM9torWVkU",
    "outputId": "06cbcfa9-38b1-4561-b31c-be1cffe3c554"
   },
   "outputs": [],
   "source": [
    "# Perform linear regression on movies_enc\n",
    "\n",
    "# import neccessary packages\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# split the data\n",
    "shuffled_idx = np.random.permutation(movies_enc.index.values)\n",
    "ntrain = (int)(.8 * len(shuffled_idx)) # 80 % of data for training)\n",
    "\n",
    "x_train = movies_enc.loc[shuffled_idx[0:ntrain],:]\n",
    "x_test = movies_enc.loc[shuffled_idx[ntrain:],:]\n",
    "y_train = gross.loc[shuffled_idx[0:ntrain]]\n",
    "y_test = gross.loc[shuffled_idx[ntrain:]]\n",
    "\n",
    "# TODO: \n",
    "# classify and predict\n",
    "# start\n",
    "\n",
    "# end your code.\n",
    "coef = clf.coef_\n",
    "intercept = clf.intercept_\n",
    "print('Model Score on 80/20 train/test split: ', score)\n",
    "print('RMSE: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIbzxpyFAfKu"
   },
   "source": [
    "Visually, we can also observe the difference in predictions and real ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "WTQirr63Ad_j",
    "outputId": "58b135ef-0d42-4689-fe6b-212a81d697f4"
   },
   "outputs": [],
   "source": [
    "# scatter\n",
    "plt.scatter(y_pred, y_test, s=.1)\n",
    "\n",
    "# mx + b regressed line\n",
    "x = np.linspace(0, 4e8, 1000)\n",
    "plt.plot(x, score * x, c='r') # R^2 Line\n",
    "plt.title('Predictions vs. Real for $gross')\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Real')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOYbn0irDdq2"
   },
   "source": [
    "### Free Response Question:\n",
    "\n",
    "How can we improve this model? Is Linear Regression the best model for this dataset?\n",
    "Respond in 3-4 sentences.\n",
    "\n",
    "_your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44n2e0nsgwrz"
   },
   "source": [
    "## You reached the end of this part of the homework!\n",
    "\n",
    "Next, we will be learning about expanding your dataset to model non-linear polynomials with featurization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdJwyUSODVew"
   },
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSDq7QT_K-0e"
   },
   "source": [
    "## Fitting an Ellipse ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdZc_m9VURf8"
   },
   "source": [
    "Lines are cool, but what if we wanted to fit something more complicated. For example, did you know that we could predict the orbit of certain planets with least squares?\n",
    "\n",
    "In fact, Carl Friedrich Gauss, a great mathematician and physicist, used least squares to predict where certain planets would be in their orbit. Using Kepler's laws of planetary motion, Gauss knew that the shape of the orbit of planets followed the equation:\n",
    "\n",
    "$$ax^{2}+bxy+cy^{2}+dx+ey=f$$\n",
    "\n",
    "where x and y were the coordinate points of the planet in orbit. Since there is one free variable in this equation, so Gauss divided the whole equation by f to get:\n",
    "\n",
    "$$ax^{2}+bxy+cy^{2}+dx+ey=1$$\n",
    "\n",
    "Now, how in the world was Gauss able to fit this equation with least squares? Let's first try to use our standard approach to least squares to solve this problem, and we'll see how that falls short. We'll generate data from an ellipse and try to reverse engineer the equation that generated it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "KY61QWa1TtyE",
    "outputId": "a031b36d-6ace-4850-89cb-a4afabba6f5f"
   },
   "outputs": [],
   "source": [
    "def ellipse_angles(n, a, b):\n",
    "    if (n <= 0 or b <= a):\n",
    "        print(\"ERROR\")\n",
    "        return\n",
    "\n",
    "    angles = 2 * np.pi * np.arange(n) / n\n",
    "    if a != b:\n",
    "        e = (1.0 - a ** 2.0 / b ** 2.0) ** 0.5\n",
    "        tot_size = sp.special.ellipeinc(2.0 * np.pi, e)\n",
    "        arc_size = tot_size / n\n",
    "        arcs = np.arange(n) * arc_size\n",
    "        res = sp.optimize.root(lambda x: (sp.special.ellipeinc(x, e) - arcs), angles)\n",
    "        angles = res.x \n",
    "    return angles\n",
    "\n",
    "a = 16\n",
    "b = 25\n",
    "n = 30\n",
    "\n",
    "phi = ellipse_angles(n, a, b)\n",
    "e = (1.0 - a ** 2.0 / b ** 2.0) ** 0.5\n",
    "arcs = sp.special.ellipeinc(phi, e)\n",
    "\n",
    "x = b * np.sin(phi)\n",
    "y = a * np.cos(phi)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.axes.set_aspect('equal')\n",
    "ax.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa0ZqQmqTudT"
   },
   "source": [
    "We have our x and y coordinates of the ellipse stored in the array x and the array y. Let's try using standard least squares to fit this ellipse. In other words, let us assume that there exists a matrix A such that\n",
    "$$cx=y$$\n",
    "where x is our array x and y is your array y. We are trying the coefficient c that best approximates this so let us do least squares. We will do this by using the closed form solution as well as directly using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjmRGqkDUZdz",
    "outputId": "3d462caf-cc25-4c5e-e2c1-fcf47d58e729"
   },
   "outputs": [],
   "source": [
    "x_train = np.array(x)\n",
    "y_train = np.array(y)\n",
    "\n",
    "### TODO\n",
    "### Your code here. Compute the least squares estimate of c using numpy.\n",
    "c = \n",
    "### Your code stops here.\n",
    "\n",
    "### TODO\n",
    "### Your code here. Compute the least squares estimate of c_sklearn using sklearn.\n",
    "### Hint: Do SKLearn Linear Regression and set fit_intercept to False.\n",
    "c_sklearn = \n",
    "### Your code stops here.\n",
    "\n",
    "print(\"Your least squares estimate for c is \" + str(c))\n",
    "print(\"Your least squares estimate for c_sklearn is \" + str(c_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yWD0x6JUfL1"
   },
   "source": [
    "Let's see how good your estimate is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "CZE7M5NIUbLq",
    "outputId": "aae651d3-8bf8-44ab-aecc-290af1573aa5"
   },
   "outputs": [],
   "source": [
    "y_predict = c*x\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.axes.set_aspect('equal')\n",
    "ax.scatter(x, y_predict)\n",
    "plt.axis([-30, 30, -5, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y35NCv59Xiez"
   },
   "source": [
    "Wow. That's a pretty bad recreation of the actual ellipse. And this should be expected. After all, least squares as we learned it can only predict straight lines. So how did Gauss fit curves with least squares? This is where featurization comes in. If the underlying relationship from our data is nonlinear, it turns out we can still deal with this through linear least-squares by augmenting the data with new features. In a sense, we will lift the dimension of each data point into a different form. In this case, since we know that the underlying structure of the model abides by the equation,\n",
    "\n",
    "$$ax^{2}+bxy+cy^{2}+dx+ey=1$$\n",
    "\n",
    ", we can now treat x and y both as datapoints and lift these datapoints to a higher dimensional space as such:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "x_1^2 & x_1y_1 & y_1^2 & x_1 & y_1\\\\\n",
    "... & ... & ... & ... & ...\\\\\n",
    "x_n^2 & ... & ... & ... & y_n\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\\\\\n",
    "d\\\\\n",
    "e\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "...\\\\\n",
    "...\\\\\n",
    "...\\\\\n",
    "1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "In this matrix equation above, we have n datapoints labeled $(x_i, y_i)$ where $i$ exists in $[1, n]$. \n",
    "\n",
    "Let's take a step back and think about what we are doing here. We have essentially lifted our 2-dimensional (x, y) datasets into a 5-dimensional vector space $(x^2, xy, y^2, x, y)$. In addition to that, we have given intrinsic structure to the shape of our points in our 5-dimensional vector space. \n",
    "\n",
    "For this specific problem, note that the regression problem has slightly changed here in that our data is now (x, y) and our ground truth output is just the number 1. With this in mind, let us do featurized linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1DFrhtvXngk"
   },
   "source": [
    "Let's first create a new augmented data matrix of size n x 5 from our x, y data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pdz-y0hcXf0M",
    "outputId": "60e7253f-b3b2-4611-fd54-9f3ff6c73776"
   },
   "outputs": [],
   "source": [
    "x_train = np.array(x)\n",
    "y_train = np.array(y)\n",
    "\n",
    "### TODO\n",
    "### Your code here. Create A_augmented.\n",
    "\n",
    "### Your code stops here.\n",
    "print(\"The first 5 rows of our new augmented matrix is \")\n",
    "print(A_augmented[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG6e7B94X4FD"
   },
   "source": [
    "Nice! Now that we have our augmented matrix, let us do least squares regression with it (via both numpy way and sklearn way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCDdE4G2X6g9",
    "outputId": "b51320fe-4ab2-4895-9a60-db5d39141959"
   },
   "outputs": [],
   "source": [
    "n = A_augmented.shape[0]\n",
    "### TODO\n",
    "### Your code here. Create the 1s vector.\n",
    "ones = \n",
    "### Your code stops here.\n",
    "\n",
    "### TODO\n",
    "### Your code here. Get the least squares estimate p using numpy.\n",
    "p = \n",
    "### Your code stops here.\n",
    "\n",
    "### TODO\n",
    "### Your code here. Get the least squares estimate p_sklearn using sklearn.\n",
    "### Hint: Do SKLearn Linear Regression and set fit_intercept to False.\n",
    "p_sklearn = \n",
    "### Your code stops here.\n",
    "\n",
    "print(\"Your least squares estimate for p is \\n\" + str(p))\n",
    "print(\"Your least squares estimate for p_sklearn is \\n\" + str(p_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzRoioaSX_kz"
   },
   "source": [
    "Now let's see how closely our predictions are to the actual values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrCrE5iqX8Pg",
    "outputId": "08cb1147-b2eb-4187-ab12-e05a5b9f3757"
   },
   "outputs": [],
   "source": [
    "print(\"Our predicted output vector is: \")\n",
    "print(A_augmented@p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9TnbrmEYFb8"
   },
   "source": [
    "## TODO ##\n",
    "You should have gotten an output of all ones. What does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBWKwYUkYYe8"
   },
   "source": [
    "## TODO ##\n",
    "\n",
    "Just for practice, we will give you a new datapoint x of value 13. Find a way to use your least squares estimate p to find the corresponding y. You may have to take out pen and paper and use an online equation solver for this now. After finding the values of y (there should be more than 1!), plug it into y_new and y_new2 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3HLTzqxYBSZ"
   },
   "outputs": [],
   "source": [
    "x_new = 13\n",
    "\n",
    "### TODO\n",
    "### Your code here. Plug in y_new.\n",
    "y_new = \n",
    "y_new_2 = \n",
    "### Your code stops here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrxQdkhEYdnc"
   },
   "source": [
    "Let's plot these two points with the others to visually see how well they fit in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Md_fukquYckW",
    "outputId": "4fa0e61b-f8f2-4f07-92c0-491cccfed5ef"
   },
   "outputs": [],
   "source": [
    "def ellipse_angles(n, a, b):\n",
    "    if (n <= 0 or b <= a):\n",
    "        print(\"ERROR\")\n",
    "        return\n",
    "\n",
    "    angles = 2 * np.pi * np.arange(n) / n\n",
    "    if a != b:\n",
    "        e = (1.0 - a ** 2.0 / b ** 2.0) ** 0.5\n",
    "        tot_size = sp.special.ellipeinc(2.0 * np.pi, e)\n",
    "        arc_size = tot_size / n\n",
    "        arcs = np.arange(n) * arc_size\n",
    "        res = sp.optimize.root(lambda x: (sp.special.ellipeinc(x, e) - arcs), angles)\n",
    "        angles = res.x \n",
    "    return angles\n",
    "\n",
    "a = 16\n",
    "b = 25\n",
    "n = 30\n",
    "\n",
    "phi = ellipse_angles(n, a, b)\n",
    "e = (1.0 - a ** 2.0 / b ** 2.0) ** 0.5\n",
    "arcs = sp.special.ellipeinc(phi, e)\n",
    "\n",
    "x = b * np.sin(phi)\n",
    "y = a * np.cos(phi)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.axes.set_aspect('equal')\n",
    "ax.scatter(x, y)\n",
    "ax.scatter([x_new, x_new], [y_new, y_new_2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9SUBIwdLYY0"
   },
   "source": [
    "### Polynomial Featurization ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4in-ggACYjcM"
   },
   "source": [
    "## TODO ##\n",
    "How do your two predicted points fit in with the rest of the points?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gkfQkBXYvVM"
   },
   "source": [
    "For polynomial featurization, it turns out sklearn has a nice shortcut for creating it. Let's check it out. Try to understand the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1DPE3A6Yg3a",
    "outputId": "b116bcac-839a-4fbc-e105-c16f1fde0dbd"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_train = np.array([[2, 3]]).reshape(1, -1)\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "x_augmented = poly_features.fit_transform(x_train)\n",
    "print(\"Degree 2 polynomial lifting of x_train: \")\n",
    "print(x_augmented)\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=3)\n",
    "x_augmented = poly_features.fit_transform(x_train)\n",
    "print(\"Degree 3 polynomial lifting of x_train: \")\n",
    "print(x_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAa-jadbY5bN"
   },
   "source": [
    "## TODO ##\n",
    "\n",
    "Explain what the degree parameter in PolynomialFeatures is doing and how the output of fit_transform relates to the input x_train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmZnAYPaY_Ce"
   },
   "source": [
    "With this nice tool equipped, let us tackle a real dataset. We will be using regression lifting in the polynomial space to predict red wine quality based on certain features such as citric acid levels and volatile acidity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "k0N28N-SYy1F",
    "outputId": "18b61423-b64a-4c43-d63d-5172c30291a6"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/winequality-red.csv')\n",
    "# We are trying to predict the quality of wine\n",
    "y = df[['quality']]\n",
    "# These are our inputs\n",
    "X = df[['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol']]\n",
    "\n",
    "# We will train on X_train/y_train and see how well our model is based on X_test/y_test\n",
    "X_train, y_train = X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtWmx77IZFpU"
   },
   "source": [
    "Feel free to play around with the X_train, y_train datasets below before we do polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYvF7F_zYww4"
   },
   "outputs": [],
   "source": [
    "### TODO\n",
    "### Play around with the data to get a feeling of what it looks like\n",
    "\n",
    "y_train\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku6ShDo8ZIJV"
   },
   "source": [
    "Now, let's do polynomial regression on X_train and y_train. Play around with the degree range in rg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ay7zcPb5ZHDz"
   },
   "outputs": [],
   "source": [
    "### TODO\n",
    "\n",
    "errors = []\n",
    "rg = [t for t in range(1, 6)]\n",
    "### Your code here. Featurize X_train and y_train using PolynomialFeatures and then do LinearRegression on it.\n",
    "for degree in rg:\n",
    "  model = \n",
    "  X_train_aug = \n",
    "\n",
    "  lg = \n",
    "  lg.fit(X_train_aug, y_train)\n",
    "\n",
    "  y_train_prediction = np.round_(lg.predict(X_train_aug))\n",
    "  err = mean_squared_error(y_train, y_train_prediction)\n",
    "  errors.append(err)\n",
    "  print(\"Training error is for degree \"  + str(degree) + \" is \" + str(err))\n",
    "### Your code stops here.\n",
    "\n",
    "plt.plot(rg, errors)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('training error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dW-EpUedaRm"
   },
   "source": [
    "## TODO ##\n",
    "The training error should monotonically decrease. Why does this happen? Does this necessarily mean that the model is becoming a better and better fit? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO ##\n",
    "Note that the exponential curve fitting you did in part 1 of the notebook is actually also a type of featurization. What is the feature map that is used to fit the curve $\\log(y)= m\\log(x) + b$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb_U1riQ10Cn"
   },
   "source": [
    "# Linear Regression for Classification #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4W4TtCl26-kz"
   },
   "source": [
    "Let's turn our attention towards another import problem in machine learning - classification. Before you proceed, pleas read the section on classification in the notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMmpsp9v16fn"
   },
   "source": [
    "We are going to use the popular `palmerpenguins` dataset which contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. Feel free to play around with the data and perform some EDA to get a feel of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "fkWDKORnZLSF",
    "outputId": "5cee3404-546a-4058-f13a-a60d978b1bcb"
   },
   "outputs": [],
   "source": [
    "df = sns.load_dataset('penguins')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQfKbYkg4sgI"
   },
   "source": [
    "## Binary Cassification ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT0-ApvT2BCk"
   },
   "source": [
    "We are going to start off by trying to classify the penguins' sex based on some of their physical dimensions. We have to assign each penguin to one of two classes: male or female. Because we have two categories, we are performing **binary classification**. Later on, we will see how we can extend this method to multiple classes. First, we have to prepare and clean our data. For this task we are going to use only two predictor variables to facilitate visualization, the penguins' *bill length* and *body mass*. \n",
    "Create two dataframes, $X$ and $y$, containing the specified columns. Remove any `NaN` values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPOlGtpU1wao"
   },
   "outputs": [],
   "source": [
    "#Remove any NaN entries and extract the relevant columns of the dataframe.\n",
    "\n",
    "#START\n",
    "X = \n",
    "y = \n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yQh53jG2Gax"
   },
   "source": [
    "To apply linear regression to this classification problems, we will have to encode the classes as numerical data. We will adopt the convention that the female class is represented by a 1 and the male class by a -1. Therefore, we are using the level set of 0 of the regression function as the classification hyperplane (refer to the notes for a more detailed explanation). Transform the data using this classification rule. Additionally, we will need a bias term so add a constant feature to the $X$ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EO7EATu2IQw"
   },
   "outputs": [],
   "source": [
    "# Create a target vector y with 1 and -1 for female and male, respectively, and add the bias term to X.\n",
    "\n",
    "#START\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "L9rhl4WV2MPg",
    "outputId": "4a59abc7-befd-4ea7-82b7-eb9b2ed11dab"
   },
   "outputs": [],
   "source": [
    "#Run this cell, no codign required.\n",
    "def visualize_dataset(X, y, title=\"\"):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(X[y < 0.0].iloc[:,0], X[y < 0.0].iloc[:,1], s=16, label='-1')\n",
    "    plt.scatter(X[y > 0.0].iloc[:,0], X[y > 0.0].iloc[:,1], s=16  , label='1')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Bill Depth, mm')\n",
    "    plt.ylabel('Body Mass, g')\n",
    "    \n",
    "visualize_dataset(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGEJw5DZ2KRe"
   },
   "source": [
    "Now we are ready to perform linear regression.  Implement the least square solution and report the weight vector and the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3XqCP7X2PYg",
    "outputId": "9bbb3b14-f22f-4684-c87f-4dc50bd28d90"
   },
   "outputs": [],
   "source": [
    "# Implement the OLS solution. You can use NumPy's linalg library.\n",
    "#START\n",
    "w = \n",
    "y_pred = \n",
    "#END\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n',w)\n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % np.mean(np.square(y - y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ftos6eB2xZ-"
   },
   "source": [
    "Now fit the model again using `sklearn`. Report the learned weights and the MSE. Do they match the results from the previos part?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QAggmxze21er",
    "outputId": "891364d2-ec93-4859-ff23-8b4a13cd64df"
   },
   "outputs": [],
   "source": [
    "#Repeat using sklearn, set fit_intercept to False\n",
    "\n",
    "#START\n",
    "\n",
    "#END\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n',w)\n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dopgYtbA24hH"
   },
   "source": [
    "As stated earlier, we are going to use 0 as our classification threshhold. We are going to classify any positive outputs of our model as 1 (Female) and any negative outputs as -1 (Male). The function in the cell below should take as input data $X$ and weights $w$ and output a vector of predicted classes. Fill in the function and report the percent of samples that were correctly classified, also known as the **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVX1JHZg28NJ",
    "outputId": "caa0c325-1545-4400-fd7c-faf3bd52cb01"
   },
   "outputs": [],
   "source": [
    "def classifier_linear(X, w):\n",
    "    \"\"\"Classify the samples X using weights w.\"\"\"\n",
    "    #START\n",
    "    return \n",
    "    #END\n",
    "\n",
    "y_pred_classes = classifier_linear(X, w)\n",
    "# Classifier accuracy\n",
    "print('Accuracy: %.2f'\n",
    "      % np.mean(y == y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKueHMC83GhZ"
   },
   "source": [
    "Now run the following cell to visualize the **decision boundary**. Briefly \n",
    "\n",
    "---\n",
    "\n",
    "comment on its shape and how well it separates the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "31QgUzad281U",
    "outputId": "b37c8f34-5728-4e81-b100-5460cdbeb4e4"
   },
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "XX, YY = np.meshgrid(np.linspace(11,23, 500), np.linspace(2500, 6500, 500))\n",
    "X_vis = np.stack([np.ravel(XX), np.ravel(YY)])\n",
    "X_vis_aug = np.ones((X_vis.shape[0]+1, X_vis.shape[1]))\n",
    "X_vis_aug[:-1,:] = X_vis\n",
    "X_vis_aug = X_vis_aug.T\n",
    "\n",
    "def visualize_classifier(X, y, w):\n",
    "    \"\"\"Visualize the decision boundary for a classifier.\n",
    "\n",
    "    Input: a vector of classification decisions using X0_aug as the samples\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10));\n",
    "    props = {'fontweight': 'bold'}\n",
    "    ZZ = np.resize(classifier_linear( X_vis_aug, w), (500, 500))\n",
    "    plt.title(\"Decision Boundary\", **props)\n",
    "    plt.contourf(XX,YY,ZZ, cmap=\"coolwarm\", levels=1)\n",
    "    plt.scatter(X.iloc[:,0], X.iloc[:,1], c=y, cmap=\"coolwarm\", s=10)\n",
    "    plt.xlabel('Bill Depth, mm')\n",
    "    plt.ylabel('Body Mass, g')\n",
    "    \n",
    "\n",
    "visualize_classifier(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9impKZR3KhG"
   },
   "source": [
    "### TODO\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d76HVZZU4im5"
   },
   "source": [
    "## Multiclass  Classification ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwIV5kl85y0r"
   },
   "source": [
    "Now, let's take a step back and look at what happens when we have multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "DQ1Qr0-m50h7",
    "outputId": "16121ffe-a90e-49c7-d3d2-637343c378be"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IG50c7qN5ync"
   },
   "source": [
    "In particular, we are going to try and model the penguins' species based on their *bill length* and and *flipper length*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5FkXoCLZ6A0T",
    "outputId": "60ca24ca-0cf9-48a8-be46-7a3127431579"
   },
   "outputs": [],
   "source": [
    "df['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "PpraAVT_6AeZ",
    "outputId": "8eaed0db-97d3-41b5-ec12-194af40d061d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10));\n",
    "gentoo = df[df['species'] == 'Gentoo'][['bill_length_mm','flipper_length_mm']]\n",
    "adelie = df[df['species'] == 'Adelie'][['bill_length_mm','flipper_length_mm']]\n",
    "chinstrap = df[df['species'] == 'Chinstrap'][['bill_length_mm','flipper_length_mm']]\n",
    "plt.scatter(gentoo['bill_length_mm'], gentoo['flipper_length_mm'], label = 'Gentoo', color = 'b')\n",
    "plt.scatter(adelie['bill_length_mm'], adelie['flipper_length_mm'], label = 'Adelie', color = 'r')\n",
    "plt.scatter(chinstrap['bill_length_mm'], chinstrap['flipper_length_mm'], label = 'Chinstrap', color = 'g')\n",
    "plt.xlabel('Bill Length, mm');\n",
    "plt.ylabel('Flipper Length, mm');\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXttK5MH5yc7"
   },
   "source": [
    "To predict three different classes, we are going to use a technique called **one-vs-all classification**. Namely, we are going to have three separate models. Each one is going to classify the samples as belonging to one of the three classes or not. Hence, the name \"one-vs-all\". At the end we are going to classify a sample as the class with the highest value outputed by the three classifiers. We will encode the Adelia, Gentoo and Chinstrap species as classes 0,1 and 2, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TreA-eN76E_-"
   },
   "outputs": [],
   "source": [
    "# Create the X and y dataframes\n",
    "X_b = df.iloc[:,[2,4]]\n",
    "X_b.insert(2, 'ones', np.ones(len(X)))\n",
    "y_b = df.iloc[:,0]\n",
    "y_orig =y_b.replace({'Adelie': 0, 'Gentoo': 1, 'Chinstrap' : 2})\n",
    "\n",
    "# Create the three different target vectors\n",
    "#START\n",
    "y1 =\n",
    "y2 =\n",
    "y3 =)\n",
    "#END\n",
    "\n",
    "#Fit the three models to each class\n",
    "#START\n",
    "\n",
    "#END\n",
    "\n",
    "# Output the final class vector based on the maximum output of the three classifiers\n",
    "#START\n",
    "y_pred = \n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfW2mYYm5yOy"
   },
   "source": [
    "Visualize the classifier's decision boundary and report its accuracy. How did it perform? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "id": "XhvQjSLG6Tvh",
    "outputId": "ed7fd09b-ea70-42eb-defe-9255c1f71b39"
   },
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "XX, YY = np.meshgrid(np.linspace(30,60, 500), np.linspace(170, 240, 500))\n",
    "X_vis = np.stack([np.ravel(XX), np.ravel(YY)])\n",
    "X_vis_aug = np.ones((X_vis.shape[0]+1, X_vis.shape[1]))\n",
    "X_vis_aug[:-1,:] = X_vis\n",
    "X_vis_aug = X_vis_aug.T\n",
    "\n",
    "def visualize_classifier_multiple(X,y, y_pred):\n",
    "    \"\"\"Visualize the decision boundary for a classifier.\n",
    "\n",
    "    Input: a vector of classification decisions using X0_aug as the samples\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10));\n",
    "    props = {'fontweight': 'bold'}\n",
    "    ZZ = np.resize(y_pred, (500, 500))\n",
    "    plt.contourf(XX,YY,ZZ, levels=2,colors = ['lightcoral','cornflowerblue','lightgreen'])\n",
    "    colors = ['darkred' if label == 0 else 'darkblue' if label == 1 else 'darkgreen' for label in y] \n",
    "    plt.scatter(X.iloc[:,0], X.iloc[:,1], c=colors, s=16)\n",
    "    plt.xlabel('Bill Length, mm');\n",
    "    plt.ylabel('Flipper Length, mm');\n",
    "\n",
    "y_pred_vis = np.argmax([X_vis_aug@w1, X_vis_aug@w2, X_vis_aug@w3],axis=0)\n",
    "visualize_classifier_multiple(X_b,y_orig, y_pred_vis)\n",
    "\n",
    "# Classifier accuracy\n",
    "print('Accuracy: %.2f'\n",
    "      % np.mean(y_orig == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTEMKNMN6X3W"
   },
   "source": [
    "### TODO\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GavQGBDy6aD5"
   },
   "source": [
    "### Exploring Modified Datasets ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2c0vYo9K3og"
   },
   "source": [
    "Now let's take a step back and examine the data from the binary case from part a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dAHxKQvI4DW"
   },
   "source": [
    "### Imbalanced Classes ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsM1uax33M1i"
   },
   "source": [
    "Now, suppose we had very few samples of male penguins. Run the following cell to produce an imbalanced dataet with very few male samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "uty1ynRi3OZa",
    "outputId": "64d2c0c1-223b-4c72-f33e-0eb77952d65d"
   },
   "outputs": [],
   "source": [
    "imbalanced = df.drop(df[(df['sex'] == 'Male')].index[0:-7])\n",
    "X_imb = imbalanced.iloc[:,[3,5]]\n",
    "y_imb = imbalanced.iloc[:,6]\n",
    "y_imb = y_imb.replace({'Female': 1, 'Male': -1})\n",
    "X_imb.insert(2, 'ones', np.ones(len(X_imb)))\n",
    "visualize_dataset(X_imb,y_imb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26YfgHwm3Qip"
   },
   "source": [
    "Perform linear regression again (feel free to use sklearn's implementation). Report the weights and the MSE. How do they compare to the previous part? Does this make intutive sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMEUw0Bb3TWc",
    "outputId": "e845dc22-df16-4f21-e905-8bb6e0bc5591"
   },
   "outputs": [],
   "source": [
    "#START\n",
    "\n",
    "#END\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n',w)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      %  np.mean(np.square(y_imb - y_pred_imb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5Lp6nK_3Xge"
   },
   "source": [
    "Run the cell below to visualize the decision boundary. Report the accuracy. Is it a good measure of the classifier performance? Is there anything wrong with the decision boundary? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "Zg3YgiZ33YCc",
    "outputId": "79ec9f86-8306-4785-8cf8-643ac8a27287"
   },
   "outputs": [],
   "source": [
    "XX, YY = np.meshgrid(np.linspace(11,23, 500), np.linspace(2500, 6500, 500))\n",
    "X_vis = np.stack([np.ravel(XX), np.ravel(YY)])\n",
    "X_vis_aug = np.ones((X_vis.shape[0]+1, X_vis.shape[1]))\n",
    "X_vis_aug[:-1,:] = X_vis\n",
    "X_vis_aug = X_vis_aug.T\n",
    "visualize_classifier(X_imb,y_imb,w)\n",
    "# Classifier accuracy\n",
    "print('Accuracy: %.2f'\n",
    "      % np.mean(y_imb == y_pred_imb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXQQGMgg3Ze7"
   },
   "source": [
    "### TODO\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5R2K0RAIzs2"
   },
   "source": [
    "### Outliers ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LWr4oZJ3bFv"
   },
   "source": [
    "Next, let's go back to the original data and consider what happens when there are outliers. In particular, let's assume there are a number of male penguins that have small bill depths and weights. Run the following cell to add this artificial data and visualzie the results. Can you guess how linear regression will perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7oQxnbO3dLz"
   },
   "outputs": [],
   "source": [
    "n_ouliers = 50\n",
    "mean = [13.0, 3000.0]\n",
    "cov = [[0.1, 0.0], [0.0, 5000]]\n",
    "X_1 = np.random.multivariate_normal(mean, cov, n_ouliers)\n",
    "X_1 = np.concatenate((X_1, np.ones((X_1.shape[0],1))),axis=1)\n",
    "y_1 = np.ones(n_ouliers)*-1\n",
    "X_out = X.append(pd.DataFrame(X_1, columns = ['bill_depth_mm', 'body_mass_g', 'ones']), ignore_index=True)\n",
    "y_out = y.append(pd.Series(y_1), ignore_index=True)\n",
    "visualize_dataset(X_out,y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMffRk3k3fwY"
   },
   "source": [
    "Perform linear regression again (feel free to use sklearn's implementation). Report the weights, the MSE, ann the accuracy. How do they compare to the previous part? Does this make intutive sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwvhMspg3hzx"
   },
   "outputs": [],
   "source": [
    "#START\n",
    "\n",
    "#END\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n',w)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y_out, y_pred))\n",
    "\n",
    "y_pred_out = classifier_linear(X_out, w)\n",
    "visualize_classifier(X_out,y_out, w)\n",
    "# Classifier accuracy\n",
    "print('Accuracy: %.2f'\n",
    "      % np.mean(y_out == y_pred_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2PUdlvIP3QV"
   },
   "source": [
    "### TODO\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6i4TNkz3kSW"
   },
   "source": [
    "Comment on the shortcommings of linear regression in these two cases. Play around with the parameters (removed samples and number of outliers) of the modified datasets and comment ot their effect. When would linear regression not be a suitable tool for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRjUtlVpPrnQ"
   },
   "source": [
    "### TODO\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples illustrate why linear regression is not a good fit (no pun intended) for classification. There are techniques which we can apply to combat these issues. For example, next week you will learn how to use the familiar OMP algorithm for outlier removal which in combination with linear regression can greatly increase the performance of our classifier. But what would be even better is if there existed models built for classification that negate these problems on their own. Luckily, they do, and you will soon see classification models such as support vector machines and logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ui_qNITPtN_"
   },
   "source": [
    "# Congratulations, you've reached the end of this assignment! #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vD7tG7XiPw_a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear Regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
